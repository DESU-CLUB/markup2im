{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='Train a diffusion model.')\n",
    "    \n",
    "    parser.add_argument('--dataset_name', type=str, default='yuntian-deng/im2latex-100k', help='Specifies which dataset to use.')\n",
    "    parser.add_argument('--input_field', type=str, default=None, help='Field in the dataset containing input markups. If set to None, will be inferred according to dataset_name.')\n",
    "    parser.add_argument('--color_mode', type=str, default=None, help='Specifies grayscale (grayscale) or RGB (rgb). If set to None, will be inferred according to dataset_name.')\n",
    "    parser.add_argument('--encoder_model_type', type=str, default=None, help='Specifies encoder model type. If set to None, will be inferred according to dataset_name.')\n",
    "    parser.add_argument('--image_height', type=int, default=None, help='Specifies the height of images to generate. If set to None, will be inferred according to dataset_name.')\n",
    "    parser.add_argument('--image_width', type=int, default=None, help='Specifies the width of images to generate. If set to None, will be inferred according to dataset_name.')\n",
    "    parser.add_argument('--save_dir', type=str, required=True, help='Output directory for saving model checkpoints.')\n",
    "    parser.add_argument('--split', type=str, default='train', help='Dataset split.')\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='Batch size.')\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Gradient accumulation steps.')\n",
    "    parser.add_argument('--num_epochs', type=int, default=10, help='Number of epochs.')\n",
    "    parser.add_argument('--scheduled_sampling_weights_start', type=float, nargs='+', default=[0,], help=\"The starting weight of applying scheduled sampling.\")\n",
    "    parser.add_argument('--scheduled_sampling_weights_end', type=float, nargs='+', default=[0.8,], help=\"The end weight of applying scheduled sampling.\")\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate.')\n",
    "    parser.add_argument('--lr_warmup_steps', type=int, default=500, help='Lr warmup steps.')\n",
    "    parser.add_argument('--clip_grad_norm', type=float, default=1.0, help='Clip gradient norm.')\n",
    "    parser.add_argument('--save_model_every', type=int, default=5, help='Saves intermediate model checkpoints every this many steps.')\n",
    "    parser.add_argument('--mixed_precision', type=str, default='no', help='Can be fp16 or no (fp32).')\n",
    "    parser.add_argument('--max_input_length', type=int, default=1024, help='Max input length. Longer inputs will be truncated.')\n",
    "    parser.add_argument('--num_dataloader_workers', type=int, default=1, help='Number of workers for dataloader.')\n",
    "    parser.add_argument('--seed1', type=int, default=42, help='Random seed for shuffling data.')\n",
    "    parser.add_argument('--seed2', type=int, default=1234, help='Random seed for data loader.')\n",
    "    \n",
    "    return parser\n",
    "\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = get_parser()\n",
    "    if args is None:\n",
    "        args = sys.argv[1:]\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def args_dict_to_list(args_dict):\n",
    "    args_list = []\n",
    "    for key, value in args_dict.items():\n",
    "        args_list.append(f'--{key}')\n",
    "        if isinstance(value, list):\n",
    "            args_list.extend(map(str, value))\n",
    "        else:\n",
    "            args_list.append(str(value))\n",
    "    return args_list\n",
    "\n",
    "# Example usage in Jupyter Notebook\n",
    "args_dict = {\n",
    "    'dataset_name': 'yuntian-deng/im2latex-100k',\n",
    "    'input_field': None,\n",
    "    'color_mode': None,\n",
    "    'encoder_model_type': None,\n",
    "    'image_height': None,\n",
    "    'image_width': None,\n",
    "    'save_dir': '/path/to/save_dir',\n",
    "    'split': 'train',\n",
    "    'batch_size': 16,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_epochs': 100,\n",
    "    'scheduled_sampling_weights_start': [0],\n",
    "    'scheduled_sampling_weights_end': [0.8],\n",
    "    'learning_rate': 1e-4,\n",
    "    'lr_warmup_steps': 500,\n",
    "    'clip_grad_norm': 1.0,\n",
    "    'save_model_every': 5,\n",
    "    'mixed_precision': 'no',\n",
    "    'max_input_length': 1024,\n",
    "    'num_dataloader_workers': 1,\n",
    "    'seed1': 42,\n",
    "    'seed2': 1234\n",
    "}\n",
    "\n",
    "# Convert dictionary to argument list and parse\n",
    "args_list = args_dict_to_list(args_dict)\n",
    "args = parse_args(args_list)\n",
    "\n",
    "# Accessing arguments\n",
    "print(f\"dataset_name: {args.dataset_name}, save_dir: {args.save_dir}, batch_size: {args.batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, '%s'%os.path.join(os.path.dirname(__file__), '../src/'))\n",
    "from markup2im_constants import get_image_size, get_input_field, get_encoder_model_type, get_color_mode\n",
    "from markup2im_models import create_image_decoder, encode_text, save_model\n",
    "import wandb\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32=True # for speed\n",
    "\n",
    "wandb.init(project=\"markup2im\")\n",
    "\n",
    "\n",
    "def train(train_dataloader, save_dir, save_model_every, \\\n",
    "        text_encoder, image_decoder, noise_scheduler, \\\n",
    "        scheduled_sampling_weights_start, scheduled_sampling_weights_end, \\\n",
    "        optimizer, lr_scheduler, num_epochs, gradient_accumulation_steps=1, \\\n",
    "        clip_grad_norm=1.0, learning_rate=1e-4, \\\n",
    "        mixed_precision='no'):\n",
    "    learning_rate = optimizer.defaults['lr']\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=mixed_precision,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "        project_dir=os.path.join(save_dir, \"logs\")\n",
    "    )\n",
    "    text_encoder = text_encoder.to(accelerator.device)\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"markup2im_train\")\n",
    "    \n",
    "    image_decoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        image_decoder, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    scheduled_sampling_weights_start = np.array(scheduled_sampling_weights_start)\n",
    "    scheduled_sampling_weights_end = np.array(scheduled_sampling_weights_end)\n",
    "    global_step = 0\n",
    "    total_steps = len(train_dataloader)*num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # the weights of scheduled sampling change linearly throughout training\n",
    "            m_probs = scheduled_sampling_weights_start + global_step / total_steps * \\\n",
    "                    (scheduled_sampling_weights_end - scheduled_sampling_weights_start)\n",
    "            m_probs = np.insert(m_probs, 0, 1-m_probs.sum()) # the weight of not applying scheduled sampling (m=0)\n",
    "            disp_str = '\"' + ' '.join([f'm={i} ({m_probs[i]:.2f})' for i in range(len(m_probs))]) + '\"'\n",
    "\n",
    "            clean_images = batch['images'].to(accelerator.device)\n",
    "            input_ids = batch['input_ids'].to(accelerator.device)\n",
    "            masks = batch['attention_mask'].to(accelerator.device)\n",
    "            encoder_hidden_states = encode_text(text_encoder, input_ids, masks)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "\n",
    "            # Sample m in scheduled sampling according to m_probs\n",
    "            # Note that we use the same m per batch on the same device for efficiency\n",
    "            m = np.random.choice(len(m_probs), size=1, p=m_probs)[0]\n",
    "\n",
    "            # If there's not enough number of steps then decrease m\n",
    "            while max(timesteps) >= noise_scheduler.num_train_timesteps-m:\n",
    "                m -= 1\n",
    "\n",
    "            # find input to the diffusion model\n",
    "            with torch.no_grad():\n",
    "                # Sample noise to add to the images\n",
    "                noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "                # first, sample t + m using Q\n",
    "                noisy_images_t_plus_m = noise_scheduler.add_noise(clean_images, noise, timesteps+m)\n",
    "                noisy_images_t_plus_s = noisy_images_t_plus_m\n",
    "                # next, roll back to t using P\n",
    "                for s in range(m):\n",
    "                    # predict noise\n",
    "                    noise_pred_rollback_s = image_decoder(noisy_images_t_plus_s, timesteps+m-s, encoder_hidden_states, attention_mask=masks)[\"sample\"]\n",
    "                    lambs_s, alpha_prod_ts_s = noise_scheduler.get_lambda_and_alpha(timesteps+m-s)\n",
    "                    # clean img predicted\n",
    "                    x_0_pred = (noisy_images_t_plus_s - lambs_s.view(-1, 1, 1, 1) * noise_pred_rollback_s) / alpha_prod_ts_s.view(-1, 1, 1, 1)\n",
    "                    noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "                    # get previous step sample\n",
    "                    noisy_images_t_plus_s_minus_one  = noise_scheduler.add_noise(x_0_pred, noise, timesteps + m-s-1)\n",
    "                    # update\n",
    "                    noisy_images_t_plus_s = noisy_images_t_plus_s_minus_one\n",
    "                noisy_images_t = noisy_images_t_plus_s\n",
    "\n",
    "            with accelerator.accumulate(image_decoder):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = image_decoder(noisy_images_t, timesteps, encoder_hidden_states, attention_mask=masks)[\"sample\"]\n",
    "                lambs_t, alpha_prod_ts_t = noise_scheduler.get_lambda_and_alpha(timesteps)\n",
    "                noise = (noisy_images_t - alpha_prod_ts_t.view(-1, 1, 1, 1) * clean_images) / lambs_t.view(-1, 1, 1, 1)\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(image_decoder.parameters(), clip_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item()*gradient_accumulation_steps, \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step, 'scheduled sampling': disp_str}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            wandb.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "        if epoch % save_model_every == 0:\n",
    "            save_model(image_decoder, os.path.join(save_dir, f'model_e{num_epochs}_lr{learning_rate}.pt.{epoch}'))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Check arguments\n",
    "    assert len(args.scheduled_sampling_weights_start) == len(args.scheduled_sampling_weights_end)\n",
    "    assert all([0 <= item <= 1 for item in args.scheduled_sampling_weights_start])\n",
    "    assert all([0 <= item <= 1 for item in args.scheduled_sampling_weights_end])\n",
    "    assert sum(args.scheduled_sampling_weights_start) <= 1\n",
    "    assert sum(args.scheduled_sampling_weights_end) <= 1\n",
    "    # Get default arguments\n",
    "    if (args.image_height is not None) and (args.image_width is not None):\n",
    "        image_size = (args.image_height, args.image_width)\n",
    "    else:\n",
    "        print (f'Using default image size for dataset {args.dataset_name}')\n",
    "        image_size = get_image_size(args.dataset_name)\n",
    "        print (f'Default image size: {image_size}')\n",
    "    args.image_size = image_size\n",
    "    if args.input_field is not None:\n",
    "        input_field = args.input_field\n",
    "    else:\n",
    "        print (f'Using default input field for dataset {args.dataset_name}')\n",
    "        input_field = get_input_field(args.dataset_name)\n",
    "        print (f'Default input field: {input_field}')\n",
    "    args.input_field = input_field\n",
    "    if args.encoder_model_type is not None:\n",
    "        encoder_model_type = args.encoder_model_type\n",
    "    else:\n",
    "        print (f'Using default encoder model type for dataset {args.dataset_name}')\n",
    "        encoder_model_type = get_encoder_model_type(args.dataset_name)\n",
    "        print (f'Default encoder model type: {encoder_model_type}')\n",
    "    args.encoder_model_type = encoder_model_type\n",
    "    if args.color_mode is not None:\n",
    "        color_mode = args.color_mode\n",
    "    else:\n",
    "        print (f'Using default color mode for dataset {args.dataset_name}')\n",
    "        color_mode = get_color_mode(args.dataset_name)\n",
    "        print (f'Default color mode: {color_mode}')\n",
    "    args.color_mode = color_mode \n",
    "    assert args.color_mode in ['grayscale', 'rgb']\n",
    "    if args.color_mode == 'grayscale':\n",
    "        args.color_channels = 1\n",
    "    else:\n",
    "        args.color_channels = 3\n",
    "\n",
    "    # Load data\n",
    "    dataset = load_dataset(args.dataset_name, split=args.split)\n",
    "    dataset = dataset.shuffle(seed=args.seed1)\n",
    "   \n",
    "    # Load input tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.encoder_model_type)\n",
    "\n",
    "    # Load input encoder\n",
    "    text_encoder = AutoModel.from_pretrained(args.encoder_model_type).cuda()\n",
    "  \n",
    "    # Preprocess data to form batches\n",
    "    transform_list = []\n",
    "    if args.color_mode == 'grayscale':\n",
    "        transform_list.append(transforms.Grayscale(num_output_channels=args.color_channels))\n",
    "    preprocess_image = transforms.Compose(\n",
    "        transform_list + [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "    def preprocess_formula(formula):\n",
    "        example = tokenizer(formula, truncation=True, max_length=args.max_input_length)\n",
    "        input_ids = example['input_ids']\n",
    "        attention_mask = example['attention_mask']\n",
    "        return input_ids, attention_mask\n",
    "    \n",
    "    def transform(examples):\n",
    "        images = [preprocess_image(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "        gold_images = [image for image in examples[\"image\"]]\n",
    "        formulas_and_masks = [preprocess_formula(formula) for formula in examples[args.input_field]]\n",
    "        formulas = [item[0] for item in formulas_and_masks]\n",
    "        masks = [item[1] for item in formulas_and_masks]\n",
    "        filenames = examples['filename']\n",
    "        return {'images': images, 'input_ids': formulas, 'attention_mask': masks, 'filenames': filenames, 'gold_images': gold_images}\n",
    "    \n",
    "    dataset.set_transform(transform)\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        eos_id = tokenizer.encode(tokenizer.eos_token)[0] # legacy code, might be unnecessary\n",
    "        max_len = max([len(example['input_ids']) for example in examples]) + 1\n",
    "        examples_out = []\n",
    "        for example in examples:\n",
    "            example_out = {}\n",
    "            orig_len = len(example['input_ids'])\n",
    "            formula = example['input_ids'] + [eos_id,] * (max_len - orig_len)\n",
    "            example_out['input_ids'] = torch.LongTensor(formula)\n",
    "            attention_mask = example['attention_mask'] + [1,] + [0,] * (max_len - orig_len - 1)\n",
    "            example_out['attention_mask'] = torch.LongTensor(attention_mask)\n",
    "            example_out['images'] = example['images']\n",
    "            examples_out.append(example_out)\n",
    "        batch = default_collate(examples_out)\n",
    "        filenames = [example['filenames'] for example in examples]\n",
    "        gold_images = [example['gold_images'] for example in examples]\n",
    "        batch['filenames'] = filenames\n",
    "        batch['gold_images'] = gold_images \n",
    "        return batch\n",
    "    \n",
    "    torch.manual_seed(args.seed2)\n",
    "    random.seed(args.seed2)\n",
    "    np.random.seed(args.seed2)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, \\\n",
    "            shuffle=True, collate_fn=collate_fn, worker_init_fn=np.random.seed(0), \\\n",
    "            num_workers=args.num_dataloader_workers)\n",
    "\n",
    "    # Create and load models\n",
    "    text_encoder = AutoModel.from_pretrained(args.encoder_model_type).cuda()\n",
    "    # forward a fake batch to figure out cross_attention_dim\n",
    "    hidden_states = encode_text(text_encoder, torch.zeros(1,1).long().cuda(), None)\n",
    "    cross_attention_dim = hidden_states.shape[-1]\n",
    "   \n",
    "    image_decoder = create_image_decoder(image_size=args.image_size, color_channels=args.color_channels, \\\n",
    "            cross_attention_dim=cross_attention_dim)\n",
    "    image_decoder = image_decoder.cuda()\n",
    "\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000, tensor_format=\"pt\")\n",
    "    # Optimization\n",
    "    optimizer = torch.optim.AdamW(image_decoder.parameters(), lr=args.learning_rate)\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_warmup_steps,\n",
    "        num_training_steps=(len(train_dataloader) * args.num_epochs),\n",
    "    )\n",
    "    train(train_dataloader, args.save_dir, args.save_model_every, \\\n",
    "        text_encoder, image_decoder, noise_scheduler, \\\n",
    "        args.scheduled_sampling_weights_start, args.scheduled_sampling_weights_end, \\\n",
    "        optimizer, lr_scheduler, args.num_epochs, \\\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps, \\\n",
    "        clip_grad_norm=args.clip_grad_norm, \\\n",
    "        mixed_precision=args.mixed_precision)\n",
    "\n",
    "    # Save final model\n",
    "    save_model(image_decoder, os.path.join(args.save_dir, f'model_e{args.num_epochs}_lr{args.learning_rate}.pt.{args.num_epochs}'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "markup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
